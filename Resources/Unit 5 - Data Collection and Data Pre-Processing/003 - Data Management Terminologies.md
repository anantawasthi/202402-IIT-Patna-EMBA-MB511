# Data Management Terminologies

## 1. A Data Frame:

In pandas, a DataFrame is a two-dimensional labeled data structure that resembles a table or spreadsheet. It consists of rows and columns, where each column can contain data of different types (e.g., integers, floats, strings) and is identified by a unique label. 

Here are some key characteristics of a DataFrame in pandas:

1. **Tabular Structure**: A DataFrame organizes data into rows and columns, similar to a spreadsheet. Each row represents a separate observation or record, while each column represents a different variable or attribute.

2. **Labeled Axes**: Both rows and columns in a DataFrame are labeled, allowing for easy identification and retrieval of data. The row labels are known as the index, while the column labels are the column names.

3. **Heterogeneous Data Types**: Unlike NumPy arrays, which require homogeneous data types, a DataFrame can hold data of different types in its columns. This flexibility allows for handling diverse datasets with ease.

4. **Size Mutability**: DataFrames are mutable, meaning you can modify their size and shape by adding or removing rows and columns dynamically. This feature is useful for data manipulation and transformation operations.

5. **Pandas Integration**: DataFrames are a core component of the pandas library, making it a powerful tool for data manipulation, analysis, and exploration in Python. Pandas provides a rich set of methods and functions for working with DataFrames efficiently.

Creating a DataFrame in pandas is straightforward. You can initialize a DataFrame from various data structures such as lists, dictionaries, NumPy arrays, or even from external data sources like CSV files or SQL databases using pandas' input/output functions.

Here's a simple example of creating a DataFrame from a dictionary:

```python
import pandas as pd

# Creating a dictionary of data
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}

# Creating a DataFrame from the dictionary
df = pd.DataFrame(data)

# Displaying the DataFrame
print(df)
```

Output:

```
      Name  Age         City
0    Alice   25     New York
1      Bob   30  Los Angeles
2  Charlie   35      Chicago
3    David   40      Houston
```

In this example, we create a DataFrame `df` from a dictionary `data`, where each key-value pair in the dictionary corresponds to a column in the DataFrame. The row index is automatically generated by pandas.

## 2. Data Import with Pandas

Importing data into pandas is a fundamental task in data analysis and manipulation. Pandas provides various functions to read data from different file formats, including CSV, Excel, JSON, SQL databases, and more. Here are examples of how to import data into pandas from some common file formats:

### 1. Importing Data from CSV File:

```python
import pandas as pd

# Read CSV file into DataFrame
df_csv = pd.read_csv('data.csv')

# Display the DataFrame
print(df_csv)
```

### 2. Importing Data from Excel File:

```python
import pandas as pd

# Read Excel file into DataFrame
df_excel = pd.read_excel('data.xlsx', sheet_name='Sheet1')

# Display the DataFrame
print(df_excel)
```

### 3. Importing Data from JSON File:

```python
import pandas as pd

# Read JSON file into DataFrame
df_json = pd.read_json('data.json')

# Display the DataFrame
print(df_json)
```

### 4. Importing Data from SQL Database:

```python
import pandas as pd
import sqlite3

# Connect to SQLite database
conn = sqlite3.connect('database.db')

# Read SQL query result into DataFrame
query = 'SELECT * FROM table_name'
df_sql = pd.read_sql_query(query, conn)

# Close database connection
conn.close()

# Display the DataFrame
print(df_sql)
```

### 5. Importing Data from Clipboard (Clipboard to DataFrame):

```python
import pandas as pd

# Read data from clipboard into DataFrame (data copied from Excel or similar)
df_clipboard = pd.read_clipboard()

# Display the DataFrame
print(df_clipboard)
```

### 6. Importing Data from HTML (Web Page to DataFrame):

```python
import pandas as pd

# Read HTML tables into list of DataFrame objects
dfs_html = pd.read_html('https://example.com/page')

# Select the desired DataFrame from the list
df_html = dfs_html[0]  # Choose the appropriate index if multiple tables are present

# Display the DataFrame
print(df_html)
```

These are some of the common methods to import data into pandas. Depending on the source and format of your data, you can choose the appropriate function to read it into a DataFrame. Additionally, these functions offer various parameters to customize the import process, such as specifying delimiter, header, index column, data types, and more.

## 3. Data Export with Pandas

In pandas, exporting data to various file formats is straightforward, and the library provides convenient functions for this purpose. Here are examples of exporting data to common file formats such as CSV, Excel, and JSON:

### Exporting to CSV:

```python
import pandas as pd

# Sample DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}
df = pd.DataFrame(data)

# Export DataFrame to CSV file
df.to_csv('data.csv', index=False)  # Set index=False to exclude row index
```

### Exporting to Excel:

```python
# Export DataFrame to Excel file
df.to_excel('data.xlsx', index=False)  # Set index=False to exclude row index
```

### Exporting to JSON:

```python
# Export DataFrame to JSON file
df.to_json('data.json', orient='records')  # Set orient='records' to export as a list of dictionaries
```

### Exporting to SQL Database:

```python
from sqlalchemy import create_engine

# Create a connection to an SQLite database
engine = create_engine('sqlite:///data.db')

# Export DataFrame to SQL database
df.to_sql('data_table', engine, index=False, if_exists='replace')
```

### Exporting to Clipboard:

```python
# Export DataFrame to clipboard (useful for copying data to other applications)
df.to_clipboard(index=False)  # Set index=False to exclude row index
```

These are just a few examples of how you can export data from a pandas DataFrame to different file formats. Depending on your requirements, you can choose the appropriate export method and customize the parameters as needed, such as specifying index options or file paths.



## 3. Subsetting Data

Dataset subsetting in pandas involves selecting a subset of data from a DataFrame based on specific conditions, indices, or column names. This is a fundamental operation in data analysis and manipulation, allowing you to focus on relevant portions of the data for further analysis. Here are some common methods for dataset subsetting in pandas with examples:

### 1. Selection by Indexing:

You can select rows and columns using integer-based indexing (`iloc`) or label-based indexing (`loc`).

```python
import pandas as pd

# Sample DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}
df = pd.DataFrame(data)

# Select rows by index range using iloc
subset1 = df.iloc[1:3]  # Select rows 1 to 2
print(subset1)

# Select rows by label using loc
subset2 = df.loc[df['Age'] > 30]  # Select rows where Age is greater than 30
print(subset2)
```

### 2. Boolean Indexing:

You can use boolean masks to filter rows based on specific conditions.

```python
# Boolean indexing
subset3 = df[df['City'] == 'New York']  # Select rows where City is 'New York'
print(subset3)
```

### 3. Selection by Column Names:

You can select specific columns by their names.

```python
# Select columns by name
subset4 = df[['Name', 'Age']]  # Select columns 'Name' and 'Age'
print(subset4)
```

### 4. Query Method:

You can use the `query()` method to filter rows based on a query expression.

```python
# Using the query method
subset5 = df.query("Age > 30")  # Select rows where Age is greater than 30
print(subset5)
```

### 5. `.iloc[]` and `.loc[]` for Row and Column Selection:

You can combine `iloc[]` and `loc[]` for more complex subsetting, selecting both rows and columns simultaneously.

```python
# Select specific rows and columns using iloc and loc
subset6 = df.iloc[1:3, 1:3]  # Select rows 1 to 2 and columns 1 to 2
print(subset6)
```

These examples demonstrate various methods for dataset subsetting in pandas. Depending on your specific requirements and the structure of your data, you can choose the appropriate method or combination of methods to extract the desired subset of data for analysis.

## Data Types conversion in Pandas

Data type conversion in pandas refers to the process of changing the data type of one or more columns in a DataFrame to another data type. This operation is useful for various data manipulation tasks, such as converting strings to numeric types, dates to timestamps, or categorical variables to a specific data type. Here's how you can perform data type conversion in pandas with examples:

### 1. Numeric Data Conversion:

You can convert columns containing numeric data (e.g., integers or floats) to another numeric data type using the `astype()` method.

```python
import pandas as pd

# Sample DataFrame
data = {
    'A': ['1', '2', '3', '4'],
    'B': ['5.1', '6.2', '7.3', '8.4']
}
df = pd.DataFrame(data)

# Convert column 'A' from string to integer
df['A'] = df['A'].astype(int)

# Convert column 'B' from string to float
df['B'] = df['B'].astype(float)

print(df.dtypes)
```

Output:

```
A      int64
B    float64
dtype: object
```

### 2. Date and Time Conversion:

You can convert columns containing date or time data to pandas' datetime data type using the `to_datetime()` function.

```python
# Sample DataFrame with dates
data = {
    'Date': ['2022-01-01', '2022-01-02', '2022-01-03'],
    'Value': [100, 200, 300]
}
df = pd.DataFrame(data)

# Convert 'Date' column to datetime data type
df['Date'] = pd.to_datetime(df['Date'])

print(df.dtypes)
```

Output:

```
Date     datetime64[ns]
Value             int64
dtype: object
```

### 3. Categorical Data Conversion:

You can convert columns containing categorical data to pandas' categorical data type using the `astype()` method or `astype('category')`.

```python
# Sample DataFrame with categorical data
data = {
    'Category': ['A', 'B', 'A', 'C']
}
df = pd.DataFrame(data)

# Convert 'Category' column to categorical data type
df['Category'] = df['Category'].astype('category')

print(df.dtypes)
```

Output:

```
Category    category
dtype: object
```

### 4. Custom Data Type Conversion:

You can perform custom data type conversion using functions like `apply()` or `map()`.

```python
# Sample DataFrame
data = {
    'A': ['1', '2', '3', '4']
}
df = pd.DataFrame(data)

# Custom conversion function
def custom_conversion(x):
    return int(x) + 10

# Apply custom conversion to column 'A'
df['A'] = df['A'].apply(custom_conversion)

print(df['A'])
```

Output:

```
0    11
1    12
2    13
3    14
Name: A, dtype: int64
```

These examples illustrate various ways to perform data type conversion in pandas. Depending on your specific requirements and the structure of your data, you can choose the appropriate method to convert columns to the desired data type.

## Operators in Pandas

In pandas, you can use logical and asthmatic operators to perform conditional operations on DataFrames and Series. These operators allow you to filter, select, and manipulate data based on specific conditions. Let's discuss how you can use logical and asthmatic operators in pandas with examples:

### Logical Operators:

Logical operators in pandas include:

- `&` for element-wise AND
- `|` for element-wise OR
- `~` for element-wise NOT

Here's how you can use logical operators to filter rows based on multiple conditions:

```python
import pandas as pd

# Sample DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}
df = pd.DataFrame(data)

# Filter rows where Age is greater than 30 AND City is 'Chicago'
filtered_df = df[(df['Age'] > 30) & (df['City'] == 'Chicago')]
print(filtered_df)
```

Output:

```
      Name  Age     City
2  Charlie   35  Chicago
```

### Asthmatic Operators:

Asthmatic operators in pandas include:

- `+` for addition
- `-` for subtraction
- `*` for multiplication
- `/` for division
- `//` for floor division
- `%` for modulus

Here's an example of using asthmatic operators to perform arithmetic operations on DataFrame columns:

```python
# Add 5 to each value in the 'Age' column
df['Age'] = df['Age'] + 5
print(df)
```

Output:

```
      Name  Age         City
0    Alice   30     New York
1      Bob   35  Los Angeles
2  Charlie   40      Chicago
3    David   45      Houston
```

You can also use asthmatic operators with scalar values or with other columns to perform element-wise arithmetic operations.

```python
# Subtract 'Age' column from 100
df['Age_diff'] = 100 - df['Age']
print(df)
```

Output:

```
      Name  Age         City  Age_diff
0    Alice   30     New York        70
1      Bob   35  Los Angeles        65
2  Charlie   40      Chicago        60
3    David   45      Houston        55
```

These examples demonstrate how you can use logical and asthmatic operators in pandas to perform conditional operations and arithmetic operations on DataFrames and Series. These operators are powerful tools for data manipulation and analysis in pandas.

## Applying functions with Pandas

In pandas, you can apply functions to DataFrames and Series using various methods like `apply()`, `applymap()`, and `map()`. These methods allow you to perform custom operations, transformations, or calculations on elements of your DataFrame or Series. Let's explore each of these methods with examples:

### 1. `apply()` method:

The `apply()` method is used to apply a function along the axis of a DataFrame or Series.

#### Example with DataFrame:

```python
import pandas as pd

# Sample DataFrame
data = {
    'A': [1, 2, 3, 4],
    'B': [5, 6, 7, 8]
}
df = pd.DataFrame(data)

# Define a custom function
def square(x):
    return x ** 2

# Apply the function to each element of DataFrame
result = df.apply(square)
print(result)
```

Output:

```
    A   B
0   1  25
1   4  36
2   9  49
3  16  64
```

#### Example with Series:

```python
# Sample Series
s = pd.Series([1, 2, 3, 4])

# Apply a function to each element of Series
result = s.apply(square)
print(result)
```

Output:

```
0     1
1     4
2     9
3    16
dtype: int64
```

### 2. `applymap()` method:

The `applymap()` method is used to apply a function element-wise to a DataFrame.

```python
# Apply a function element-wise to DataFrame
result = df.applymap(square)
print(result)
```

Output:

```
    A   B
0   1  25
1   4  36
2   9  49
3  16  64
```

### 3. `map()` method:

The `map()` method is used to apply a function element-wise to each element of a Series.

```python
# Apply a function element-wise to Series
result = s.map(square)
print(result)
```

Output:

```
0     1
1     4
2     9
3    16
dtype: int64
```

### Using Lambda Functions:

You can also use lambda functions with these methods for concise operations:

```python
# Applying a lambda function to Series
result = s.apply(lambda x: x ** 2)
print(result)
```

Output:

```
0     1
1     4
2     9
3    16
dtype: int64
```

```python
# Applying a lambda function to DataFrame
result = df.applymap(lambda x: x ** 2)
print(result)
```

Output:

```
    A   B
0   1  25
1   4  36
2   9  49
3  16  64
```

These examples demonstrate how you can apply functions to pandas DataFrames and Series using `apply()`, `applymap()`, and `map()` methods. These methods are versatile tools for performing custom operations and transformations on your data.

## Control Structure in Pandas

In pandas, you can apply control statements like `if`, `else`, `elif`, `for`, and `while` loops in conjunction with conditional expressions to perform various tasks such as data filtering, conditional data assignment, or iterative operations. Here's how you can use control statements in pandas:

### 1. Filtering Data with `if`, `else`, and `elif`:

You can use control statements to filter data based on specific conditions.

```python
import pandas as pd

# Sample DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40]
}
df = pd.DataFrame(data)

# Filtering data based on age
for index, row in df.iterrows():
    if row['Age'] < 30:
        df.loc[index, 'Category'] = 'Young'
    elif row['Age'] >= 30 and row['Age'] < 40:
        df.loc[index, 'Category'] = 'Middle-aged'
    else:
        df.loc[index, 'Category'] = 'Old'

print(df)
```

Output:

```
      Name  Age     Category
0    Alice   25        Young
1      Bob   30  Middle-aged
2  Charlie   35  Middle-aged
3    David   40          Old
```

### 2. Iterating Over Rows with `iterrows()`:

You can iterate over rows in a DataFrame using the `iterrows()` method.

```python
# Iterating over rows and printing Name and Age
for index, row in df.iterrows():
    print(row['Name'], row['Age'])
```

### 3. Using `apply()` with a Custom Function:

You can also apply control statements within custom functions and use `apply()` to apply them to DataFrame or Series.

```python
# Define a custom function to categorize age
def categorize_age(age):
    if age < 30:
        return 'Young'
    elif age >= 30 and age < 40:
        return 'Middle-aged'
    else:
        return 'Old'

# Apply the function to create a new column 'Category'
df['Category'] = df['Age'].apply(categorize_age)

print(df)
```

### 4. Using List Comprehension:

List comprehension can also be used to conditionally assign values to DataFrame columns.

```python
# Using list comprehension to create a new column 'Category'
df['Category'] = ['Young' if age < 30 else 'Middle-aged' if age < 40 else 'Old' for age in df['Age']]

print(df)
```

These examples demonstrate how you can apply control statements in pandas for filtering data, iterating over rows, and performing conditional operations using methods like `iterrows()`, `apply()`, and list comprehension. Control statements can be powerful tools for data manipulation and analysis in pandas when used effectively.

## Sorting in Pandas

In pandas, you can sort DataFrame or Series data based on one or more columns using various sorting methods. Let's discuss different sorting methods in pandas along with examples:

### 1. `sort_values()` method:

The `sort_values()` method is used to sort DataFrame by the values along either axis.

#### Sorting DataFrame by a Single Column:

```python
import pandas as pd

# Sample DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 30]
}
df = pd.DataFrame(data)

# Sorting DataFrame by 'Age' column in ascending order
sorted_df = df.sort_values(by='Age')
print(sorted_df)
```

Output:

```
      Name  Age
0    Alice   25
1      Bob   30
3    David   30
2  Charlie   35
```

#### Sorting DataFrame by Multiple Columns:

```python
# Sorting DataFrame by 'Age' and 'Name' columns
sorted_df = df.sort_values(by=['Age', 'Name'], ascending=[True, False])
print(sorted_df)
```

Output:

```
      Name  Age
0    Alice   25
3    David   30
1      Bob   30
2  Charlie   35
```

### 2. `sort_index()` method:

The `sort_index()` method is used to sort DataFrame by index labels.

```python
# Sorting DataFrame by index labels
sorted_df = df.sort_index()
print(sorted_df)
```

### 3. Sorting Series:

You can also sort Series using the `sort_values()` method.

```python
# Sample Series
s = pd.Series([3, 1, 4, 2], index=['c', 'a', 'd', 'b'])

# Sorting Series by values
sorted_s = s.sort_values()
print(sorted_s)
```

Output:

```
a    1
b    2
c    3
d    4
dtype: int64
```

### 4. `nlargest()` and `nsmallest()` methods:

These methods are used to get the n largest or n smallest values from a Series or DataFrame.

```python
# Getting 2 largest values from 'Age' column
largest_values = df['Age'].nlargest(2)
print(largest_values)

# Getting 2 smallest values from 'Age' column
smallest_values = df['Age'].nsmallest(2)
print(smallest_values)
```

Output:

```
2    35
1    30
Name: Age, dtype: int64
0    25
1    30
Name: Age, dtype: int64
```

These are some of the common methods for sorting data in pandas. Sorting is an essential operation for organizing and analyzing data effectively, and pandas provides versatile methods to perform sorting based on various criteria.

## Merging Methods in Pandas

In pandas, data merge and joining operations allow you to combine datasets based on one or more common columns or indices. These operations are useful for integrating data from multiple sources or merging datasets with related information. Let's discuss data merge and joining in pandas with examples:

### 1. `pd.merge()` function:

The `pd.merge()` function is used to merge two DataFrames based on one or more common columns. By default, it performs an inner join.

```python
import pandas as pd

# Sample DataFrames
df1 = pd.DataFrame({'ID': [1, 2, 3],
                    'Name': ['Alice', 'Bob', 'Charlie']})
df2 = pd.DataFrame({'ID': [2, 3, 4],
                    'Age': [25, 30, 35]})

# Merge DataFrames based on 'ID' column
merged_df = pd.merge(df1, df2, on='ID')
print(merged_df)
```

Output:

```
   ID    Name  Age
0   2     Bob   25
1   3  Charlie   30
```

### 2. `pd.concat()` function:

The `pd.concat()` function is used to concatenate two or more DataFrames along a specified axis.

```python
# Concatenate DataFrames along rows (axis=0)
concatenated_df = pd.concat([df1, df2], ignore_index=True)
print(concatenated_df)
```

Output:

```
   ID     Name   Age
0   1    Alice   NaN
1   2      Bob   NaN
2   3  Charlie   NaN
3   2      NaN  25.0
4   3      NaN  30.0
5   4      NaN  35.0
```

### 3. `df.join()` method:

The `df.join()` method is used to join two DataFrames on their indices.

```python
# Set 'ID' column as index in both DataFrames
df1.set_index('ID', inplace=True)
df2.set_index('ID', inplace=True)

# Join DataFrames on their indices
joined_df = df1.join(df2, lsuffix='_left', rsuffix='_right')
print(joined_df)
```

Output:

```
     Name  Age
ID            
1   Alice  NaN
2     Bob   25
3  Charlie   30
```

### 4. `pd.merge()` with Different Types of Joins:

You can specify different types of joins (inner, outer, left, right) using the `how` parameter.

```python
# Performing a left join
left_join_df = pd.merge(df1, df2, on='ID', how='left')
print(left_join_df)
```

Output:

```
   ID     Name   Age
0   1    Alice   NaN
1   2      Bob  25.0
2   3  Charlie  30.0
```

These are some common methods for performing data merge and joining operations in pandas. Depending on your specific requirements and the structure of your datasets, you can choose the appropriate method to integrate data effectively.

## Do and Don'ts while running joins

When running joins in pandas, there are certain best practices and considerations to keep in mind to ensure efficient and accurate results. Here are some do's and don'ts when performing joins in pandas:

### Do's:

1. **Understand Your Data**: Before performing a join, thoroughly understand the structure and content of your datasets, including column names, data types, and potential duplicates.

2. **Use Appropriate Join Types**: Choose the appropriate join type (inner, outer, left, right) based on your analysis requirements and the relationship between the datasets.

3. **Check for Duplicate Keys**: Ensure that the join keys are unique in both datasets to avoid unexpected behavior and duplicate rows in the result.

4. **Handle Missing Values**: Handle missing values appropriately before performing the join, either by filling them with default values or dropping them if appropriate.

5. **Set Indices for Efficient Joining**: Set indices on the joining columns if they are frequently used for joining, as it can significantly improve the performance of join operations.

6. **Test on Small Samples**: Before applying joins to large datasets, test the join operation on small samples to verify correctness and assess performance.

7. **Use Merge Functions Judiciously**: Familiarize yourself with different merge functions (e.g., `pd.merge()`, `df.join()`, `pd.concat()`) and choose the one that best suits your use case.

### Don'ts:

1. **Assume Default Behavior**: Avoid assuming default behavior for join operations; always specify join type and other parameters explicitly to avoid unexpected results.

2. **Ignore Potential Data Mismatches**: Validate the data consistency between the joining columns in both datasets to avoid mismatched or incorrect results.

3. **Join Without Understanding Data Size**: Understand the size of your datasets and the potential impact of the join operation on memory usage and computational resources.

4. **Overcomplicate Join Conditions**: Keep join conditions simple and straightforward; avoid overly complex conditions that can make the code difficult to understand and maintain.

5. **Use Joins for Unrelated Data**: Avoid using joins for unrelated datasets or when a more appropriate method (such as merging on index) could achieve the desired result more efficiently.

6. **Forget to Reset Index**: If you're using index-based joins (`df.join()`), remember to reset the index if necessary to avoid unexpected behavior due to mismatched indices.

7. **Overlook Performance Optimization**: Optimize join performance by using appropriate data types, setting indices, and avoiding unnecessary computations or memory overhead.

By following these do's and don'ts, you can ensure that your join operations in pandas are performed accurately, efficiently, and with minimal risk of errors or unexpected behavior.



## Reshaping Data in Pandas

Data reshaping in pandas involves restructuring the layout of data in a DataFrame to facilitate analysis, visualization, and modeling. It includes operations like pivoting, melting, stacking, and unstacking, which help transform data from one format to another. Let's discuss data reshaping techniques in pandas with examples:

### 1. Pivoting:

Pivoting reshapes data by rotating the rows into columns, and vice versa, based on one or more columns' values.

#### Example:

```python
import pandas as pd

# Sample DataFrame
data = {
    'Date': ['2022-01-01', '2022-01-01', '2022-01-02', '2022-01-02'],
    'Category': ['A', 'B', 'A', 'B'],
    'Value': [10, 20, 30, 40]
}
df = pd.DataFrame(data)

# Pivot the DataFrame
pivot_df = df.pivot(index='Date', columns='Category', values='Value')
print(pivot_df)
```

Output:

```
Category       A     B
Date                  
2022-01-01  10.0  20.0
2022-01-02  30.0  40.0
```

### 2. Melting:

Melting reshapes data from wide to long format by unpivoting it.

#### Example:

```python
# Melting the DataFrame
melted_df = df.melt(id_vars='Date', value_vars=['A', 'B'], var_name='Category', value_name='Value')
print(melted_df)
```

Output:

```
         Date Category  Value
0  2022-01-01        A     10
1  2022-01-01        B     20
2  2022-01-02        A     30
3  2022-01-02        B     40
```

### 3. Stacking and Unstacking:

Stacking and unstacking reshape data by pivoting the innermost level of column labels.

#### Example:

```python
# Sample DataFrame with MultiIndex columns
data = {
    ('A', 'X'): [1, 2, 3],
    ('A', 'Y'): [4, 5, 6],
    ('B', 'X'): [7, 8, 9],
    ('B', 'Y'): [10, 11, 12]
}
df = pd.DataFrame(data)

# Stacking the DataFrame
stacked_df = df.stack()
print(stacked_df)
```

Output:

```
       X   Y
0 A  1  4
  B  7  10
1 A  2  5
  B  8  11
2 A  3  6
  B  9  12
```

```python
# Unstacking the DataFrame
unstacked_df = stacked_df.unstack()
print(unstacked_df)
```

Output:

```
     X       Y
0  1.0     4.0
1  2.0     5.0
2  3.0     6.0
```

### 4. Reshaping with `stack()` and `unstack()`:

These methods are used for reshaping MultiIndex DataFrames.

#### Example:

```python
# Stacking the DataFrame along level 0
stacked_df = df.stack(level=0)
print(stacked_df)
```

Output:

```
           X    Y
0 A  1.0  4.0
  B  7.0  10.0
1 A  2.0  5.0
  B  8.0  11.0
2 A  3.0  6.0
  B  9.0  12.0
```

```python
# Unstacking the DataFrame along level 1
unstacked_df = stacked_df.unstack(level=1)
print(unstacked_df)
```

Output:

```
     X        Y
0  1.0     4.0
1  2.0     5.0
2  3.0     6.0
```

These examples illustrate various data reshaping techniques in pandas, including pivoting, melting, stacking, and unstacking. Understanding and using these methods effectively can help you manipulate and analyze your data more efficiently.

## Data Aggregation in Pandas

Data aggregation in pandas involves combining and summarizing data across one or more dimensions, typically using aggregation functions like sum, mean, count, etc. This process is essential for gaining insights into large datasets and summarizing them in a meaningful way. Let's discuss data aggregation in pandas in detail with examples:

### 1. Using `groupby()` for Aggregation:

The `groupby()` function in pandas is one of the primary tools for data aggregation. It allows you to group data based on one or more columns and then apply aggregation functions to these groups.

#### Example:

```python
import pandas as pd

# Sample DataFrame
data = {
    'Category': ['A', 'B', 'A', 'B', 'A'],
    'Value': [10, 20, 30, 40, 50]
}
df = pd.DataFrame(data)

# Grouping by 'Category' and calculating the sum of 'Value' within each group
aggregated_df = df.groupby('Category').sum()
print(aggregated_df)
```

Output:

```
          Value
Category       
A            90
B            60
```

### 2. Applying Multiple Aggregation Functions:

You can apply multiple aggregation functions simultaneously using the `agg()` method.

#### Example:

```python
# Applying multiple aggregation functions
aggregated_df = df.groupby('Category').agg({'Value': ['sum', 'mean', 'count']})
print(aggregated_df)
```

Output:

```
         Value           
           sum mean count
Category                 
A           90   30     3
B           60   30     2
```

### 3. Using Pivot Tables for Aggregation:

Pivot tables in pandas provide another way to perform data aggregation, especially for summarizing data across multiple dimensions.

#### Example:

```python
# Creating a pivot table to summarize data
pivot_table = df.pivot_table(index='Category', aggfunc={'Value': ['sum', 'mean', 'count']})
print(pivot_table)
```

Output:

```
         Value           
           count mean sum
Category                 
A              3   30  90
B              2   30  60
```

### 4. Custom Aggregation Functions:

You can define custom aggregation functions using lambda functions or regular functions and apply them to grouped data.

#### Example:

```python
# Custom aggregation function to calculate the range
range_func = lambda x: x.max() - x.min()

# Applying custom aggregation function
custom_agg_df = df.groupby('Category')['Value'].agg(['sum', 'mean', range_func])
print(custom_agg_df)
```

Output:

```
          sum  mean  <lambda>
Category                      
A           90    30        40
B           60    30        20
```

### 5. Using `transform()` for Aggregation:

The `transform()` function can be used to perform aggregation while preserving the original shape of the DataFrame.

#### Example:

```python
# Calculating the mean of each group and broadcasting it to the original DataFrame
df['Mean_Value'] = df.groupby('Category')['Value'].transform('mean')
print(df)
```

Output:

```
  Category  Value  Mean_Value
0        A     10          30
1        B     20          30
2        A     30          30
3        B     40          30
4        A     50          30
```

These examples demonstrate various methods for performing data aggregation in pandas, including `groupby()`, `pivot_table()`, custom aggregation functions, and `transform()`. By leveraging these techniques, you can summarize and analyze your datasets effectively, gaining valuable insights from your data.

## Data Pivoting in Pandas

**Chapter: Pivoting in Pandas**

Pivoting is a powerful technique in pandas for reshaping and transforming data, particularly when you want to summarize or aggregate data across multiple dimensions. In this chapter, we will explore the concept of pivoting in pandas in detail, covering its syntax, applications, and examples.

**1. Introduction to Pivoting:**

Pivoting in pandas involves transforming data from a long format to a wide format or vice versa. It allows you to reorganize and reshape your data, making it easier to analyze and understand. Pivoting is commonly used in data analysis, reporting, and visualization tasks.

**2. Syntax of Pivoting:**

The main function for pivoting in pandas is `pivot_table()`, although the `pivot()` function is also available for simple pivoting operations. Here's the syntax for `pivot_table()`:

```python
DataFrame.pivot_table(values=None, index=None, columns=None, aggfunc='mean', fill_value=None)
```

- `values`: Column to aggregate.
- `index`: Column(s) to group by as rows.
- `columns`: Column(s) to group by as columns.
- `aggfunc`: Aggregation function to apply (default is 'mean').
- `fill_value`: Replace missing values with this value.

**3. Pivoting Examples:**

**Example 1: Simple Pivot Table**

```python
import pandas as pd

# Sample DataFrame
data = {
    'Date': ['2022-01-01', '2022-01-01', '2022-01-02', '2022-01-02'],
    'Category': ['A', 'B', 'A', 'B'],
    'Value': [10, 20, 30, 40]
}
df = pd.DataFrame(data)

# Pivot table to summarize 'Value' by 'Category' and 'Date'
pivot_table = df.pivot_table(values='Value', index='Date', columns='Category', aggfunc='sum')
print(pivot_table)
```

**Example 2: Pivot Table with Aggregation Functions**

```python
# Pivot table with multiple aggregation functions
pivot_table = df.pivot_table(values='Value', index='Date', columns='Category', aggfunc=['sum', 'mean'])
print(pivot_table)
```

**Example 3: Handling Missing Values**

```python
# Pivot table with handling of missing values
pivot_table = df.pivot_table(values='Value', index='Date', columns='Category', aggfunc='sum', fill_value=0)
print(pivot_table)
```

**Example 4: Reverse Pivoting**

```python
# Reverse pivoting (from wide to long format)
df_long = pivot_table.reset_index().melt(id_vars='Date', value_vars=['A', 'B'], var_name='Category', value_name='Value')
print(df_long)
```

**4. Key Considerations and Best Practices:**

- **Data Understanding**: Understand your data structure and the desired outcome before performing pivoting operations.
- **Choosing Aggregation Functions**: Choose appropriate aggregation functions based on the analysis goals and the nature of the data.
- **Handling Missing Values**: Decide on how to handle missing values, either by filling them with a default value or ignoring them during aggregation.
- **Reshaping Data**: Consider the shape of the data after pivoting and whether it suits the analysis and visualization requirements.
- **Performance**: Be mindful of the performance implications, especially when dealing with large datasets or complex pivoting operations.

**5. Conclusion:**

Pivoting is a versatile and powerful technique in pandas for reshaping and summarizing data. By understanding its syntax, applications, and best practices, you can effectively transform and analyze your datasets to derive valuable insights. Experiment with different pivoting operations to see how they can help you gain deeper insights into your data.
